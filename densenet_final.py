# -*- coding: utf-8 -*-
"""DenseNet_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w8GXBxbfu4AeMvEHD40aw3FtP_98AnHN
"""

pip install pytorch-ignite

# imports
import numpy as np
import random
from matplotlib import pyplot as plt
import torch
from torchvision import datasets, transforms
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset, ConcatDataset
from torchvision import models
from ignite.engine import Engine, Events, create_supervised_evaluator, create_supervised_trainer
from ignite.metrics import Accuracy, Loss, RunningAverage, ConfusionMatrix
from torch.utils.data.sampler import SubsetRandomSampler
from datetime import datetime
from sklearn.metrics import confusion_matrix
from sklearn import metrics 
import time

# plot the confusion matrix
def plot_confusion_matrix(cm, epochs, densenet, normalize = True, cmap = plt.cm.Reds):
    fig = plt.figure(figsize = (8,8))
    plt.imshow(cm, interpolation = 'nearest',  cmap = cmap)
    plt.title(f'Confusion matrix DenseNet {densenet}, {epochs} epoci', fontsize = 18)
    plt.xticks(range(20), fontsize=14)
    plt.yticks(range(20), fontsize=14)
    plt.colorbar()
    plt.show()

# calculate execution time in minutes and seconds
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

# declare some variables
superClasses = True
test_percent = 10
batch_size = 128
n_classes = 20 if superClasses else 100
elements_per_class = 3000 if superClasses else 600
transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor()])

# reading the dataset
torch.manual_seed(42)  
train_set = datasets.CIFAR100('data', train = True, transform=transform, download=True) # read train set
test_set = datasets.CIFAR100('data', train = False, transform=transform, download=True) # read test set
dataset = ConcatDataset([train_set, test_set]) # concatenate the sets

all_classes = [
                ['beaver', 'dolphin', 'otter', 'seal', 'whale'],
                ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],
                ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],
                ['bottle', 'bowl', 'can', 'cup', 'plate'],
                ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],
                ['clock', 'keyboard', 'lamp', 'telephone', 'television'],
                ['bed', 'chair', 'couch', 'table', 'wardrobe'],
                ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],
                ['bear', 'leopard', 'lion', 'tiger', 'wolf'],
                ['bridge', 'castle', 'house', 'road', 'skyscraper'],
                ['cloud', 'forest', 'mountain', 'plain', 'sea'],
                ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],
                ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],
                ['crab', 'lobster', 'snail', 'spider', 'worm'],
                ['baby', 'boy', 'girl', 'man', 'woman'],
                ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],
                ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],
                ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],
                ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],
                ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']
            ]
            
all_indexes = {}               
for index, elements in enumerate(all_classes):
    indexes = []
    for elem in elements:
        indexes.append(train_set.class_to_idx[elem])
    all_indexes[index] = indexes

# organize images per class/superclass
all_classes_data = []
if superClasses:
    for index in range(n_classes):
        class_Set = []
        for elem in range(len(dataset)):            
            if dataset[elem][1] in all_indexes[index]:
                element = (dataset[elem][0], int(index))
                class_Set.append(element) 
        all_classes_data.append(class_Set)
else:
    for index in range(n_classes):
        class_Set = []
        for elem in range(len(dataset)):
            if dataset[elem][1] == index:
                class_Set.append(dataset[elem])
        all_classes_data.append(class_Set)

data = ConcatDataset(all_classes_data)

# split the dataset in test and train, make train and test loaders
random_train = random.sample(range(elements_per_class), elements_per_class - int(test_percent*elements_per_class/100))
random_test = []
for x in range(elements_per_class):
    if x not in random_train:
        random_test.append(x)
train_indices, test_indices = random_train, random_test
for i in range(1, n_classes):
    random_train = [x + elements_per_class for x in random_train]
    random_test = [x + elements_per_class for x in random_test]
    train_indices.extend(random_train)
    test_indices.extend(random_test)

print(f"Numarul de imagini: {len(dataset)}")
print(f"Numarul de imagini de antrenare ({100-test_percent}%): {len(train_indices)}")
print(f"Numarul de imagini de test ({test_percent}%): {len(test_indices)}")
print(f"Numarul de clase: {n_classes}")

train_sampler = SubsetRandomSampler(train_indices)
test_sampler = SubsetRandomSampler(test_indices)
train_loader = DataLoader(data, batch_size=batch_size, num_workers=2, pin_memory=True, sampler=train_sampler)
test_loader  = DataLoader(data, batch_size=batch_size, num_workers=2, pin_memory=True, sampler=test_sampler)

# declare some variables
epochs = 25
dense = 201
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# defining DenseNet models
if dense == 121:
  model = models.densenet121(pretrained=True)
elif dense == 169:
  model = models.densenet169(pretrained=True)
else:
  model = models.densenet201(pretrained=True)

for param in model.parameters():
    param.requires_grad = False

n_inputs = model.classifier.in_features
last_layer = nn.Linear(n_inputs, n_classes)
model.classifier = last_layer
# print(model.classifier.out_features)
model = model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.classifier.parameters())

# train the training set
accuracy_train = [] 
trainer = create_supervised_trainer(model, optimizer, criterion, device=device) # train the training set
evaluator = create_supervised_evaluator(model, device=device, metrics={'accuracy' : Accuracy()} # evaluate the training set

@trainer.on(Events.EPOCH_COMPLETED) # show details on each epoch
def log_training_results(trainer):
    evaluator.run(train_loader)
    metrics = evaluator.state.metrics
    accuracy_train.append(metrics['accuracy']*100)
    print(f"Epoca {trainer.state.epoch}/{epochs} - Acuratete: {metrics['accuracy']*100:.2f}")

print("Antrenare:")
start_train = time.monotonic()
trainer.run(train_loader, max_epochs = epochs) # train and evaluate the training set
end_train = time.monotonic()
epoch_mins_train, epoch_secs_train = epoch_time(start_train, end_train)
print(f'Timp de executie antrenare: : {epoch_mins_train}m {epoch_secs_train}s')

for i in accuracy_train:
  print(f"{i:.2f}, ", end = '')

# test the testing set
print("Test:")
start_test = time.monotonic()

true_label, predict_label = [], []
model.eval() # setting model to evaluation mode
for data, target in test_loader:
    output = model(data) # test the model  
    _, pred = torch.max(output, 1)
    for elem in pred:
        predict_label.append(int(elem))
    for elem in target.data:
        true_label.append(int(elem))

count = 0
for i in range(len(predict_label)):
    if true_label[i] == predict_label[i]:
        count += 1
print(f"Numar de imagini de test prezise corect DenseNet {dense}: {count}/{len(true_label)} ")

accuracy_test = count*100/len(true_label)
print(f"Acuratete: {accuracy_test:.2f}")

end_test = time.monotonic()
epoch_mins_test, epoch_secs_test = epoch_time(start_test, end_test)
print(f'Timp de executie antrenare: : {epoch_mins_test}m {epoch_secs_test}s')

# plot confusion matrix
confusion_mtx = confusion_matrix(true_label, predict_label)
plot_confusion_matrix(confusion_mtx, epochs, dense)